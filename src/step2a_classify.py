"""
é“¶è¡ŒåŒ¹é…é¡¹ç›® - ç¬¬äºŒæ­¥Aï¼šAIåˆ†ç±»é“¶è¡Œå®ä½“

æœ¬è„šæœ¬ä½¿ç”¨AIå¤§æ¨¡å‹å¯¹ç¬¬ä¸€æ­¥ç­›é€‰å‡ºçš„æ½œåœ¨é“¶è¡Œå®ä½“è¿›è¡Œç²¾ç¡®åˆ†ç±»ï¼Œ
åˆ¤æ–­æ¯ä¸ªå®ä½“æ˜¯å¦ä¸ºçœŸæ­£çš„ç¾å›½é“¶è¡Œæˆ–é“¶è¡Œæ§è‚¡å…¬å¸ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. è¯»å–ç¬¬ä¸€æ­¥ç”Ÿæˆçš„å”¯ä¸€å€Ÿè´·æ–¹åç§°åˆ—è¡¨
2. ä½¿ç”¨AIå¤§æ¨¡å‹å¹¶å‘å¤„ç†è¿™äº›åç§°è¿›è¡Œåˆ†ç±»
3. æ ¹æ®é¢„è®¾æ ‡å‡†åˆ¤æ–­å®ä½“æ˜¯å¦ä¸ºFDICä¿é™©çš„ç¾å›½é“¶è¡Œæˆ–é“¶è¡Œæ§è‚¡å…¬å¸
4. æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œé¿å…é‡å¤å¤„ç†å·²åˆ†ç±»çš„å®ä½“
5. å°†åˆ†ç±»ç»“æœä¿å­˜åˆ°CSVæ–‡ä»¶ä¸­ä¾›åç»­æ­¥éª¤ä½¿ç”¨

åˆ†ç±»æ ‡å‡†ï¼š
- TRUE (ä¿ç•™)ï¼šå•†ä¸šé“¶è¡Œã€å‚¨è“„é“¶è¡Œã€é“¶è¡Œæ§è‚¡å…¬å¸ã€å¤–å›½é“¶è¡Œç¾å›½å­å…¬å¸
- FALSE (ä¸¢å¼ƒ)ï¼šæŠ•èµ„åŸºé‡‘ã€ä¿é™©å…¬å¸ã€éé“¶è¡Œé‡‘èæœºæ„ã€æŠµæŠ¼è´·æ¬¾REITsç­‰

å…³é”®ç‰¹æ€§ï¼š
- ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘æé«˜å¤„ç†æ•ˆç‡
- å®ç°æ–­ç‚¹ç»­ä¼ åŠŸèƒ½ï¼Œæ”¯æŒä¸­æ–­åç»§ç»­å¤„ç†
- å…·å¤‡å¼ºå¤§çš„JSONè§£æèƒ½åŠ›ï¼Œèƒ½å¤„ç†å„ç§æ ¼å¼çš„AIå“åº”
- é€šè¿‡æ‰¹é‡å¤„ç†å‡å°‘APIè°ƒç”¨æ¬¡æ•°

ä¾èµ–ï¼š
- pandas: æ•°æ®å¤„ç†
- openai: è°ƒç”¨å¤§æ¨¡å‹API
- config: é¡¹ç›®é…ç½®æ–‡ä»¶
- concurrent.futures: å¹¶å‘å¤„ç†
- tqdm: è¿›åº¦æ¡æ˜¾ç¤º
"""


import pandas as pd
import os
import sys
import json
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from openai import OpenAI

# å¯¼å…¥ config
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config

# ================= æç¤ºè¯ =================
PROMPT_CLASSIFY = """
Role: Financial Entity Classifier.
Task: Determine if the provided entity names are likely "FDIC-insured US Banks" or "Bank Holding Companies".

Criteria for TRUE (Keep):
- Commercial Banks, Savings Banks, Thrifts.
- Bank Holding Companies (e.g., Citigroup Inc).
- US subsidiaries of foreign banks.

Criteria for FALSE (Discard):
- Investment Funds / PE Firms / Hedge Funds.
- Insurance Companies.
- Non-Bank Financial Corps (e.g., GM Financial).
- Pure Mortgage REITs or SPVs.

Output: JSON Object with a list "results": [{"name": "...", "is_bank": true/false}, ...]
IMPORTANT: Only generate valid JSON output.
"""


# ================= å·¥å…·å‡½æ•° =================
def init_client():
    if not config.ARK_API_KEY:
        raise ValueError("âŒ ARK_API_KEY æœªè®¾ç½®")
    return OpenAI(base_url="https://ark.cn-beijing.volces.com/api/v3", api_key=config.ARK_API_KEY)


def parse_json(raw_text):
    """
    å¢å¼ºç‰ˆ JSON è§£æå™¨ï¼š
    1. ä¼˜å…ˆæå– Markdown ä»£ç å—
    2. æš´åŠ›æˆªå–ç¬¬ä¸€ä¸ª '{' åˆ°æœ€åä¸€ä¸ª '}' ä¹‹é—´çš„å†…å®¹ (è§£å†³ Extra data é—®é¢˜)
    """
    if not raw_text: return None

    text = raw_text.strip()

    # 1. å°è¯•æå– Markdown ```json ... ```
    # ä½¿ç”¨éè´ªå©ªåŒ¹é…ï¼Œé˜²æ­¢åŒ¹é…åˆ°å¤šä¸ªä»£ç å—
    match = re.search(r'```(?:json)?\s*(.*?)\s*```', text, re.DOTALL)
    if match:
        text = match.group(1).strip()

    # 2. ç¬¬ä¸€æ¬¡å°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(text)
    except:
        pass

    # 3. æš´åŠ›æˆªå–ï¼šå¯»æ‰¾æœ€å¤–å±‚çš„ {} æˆ– []
    # è¿™æ˜¯è§£å†³ "Extra data" çš„ç»ˆææ–¹æ¡ˆ
    try:
        if '{' in text and '}' in text:
            start = text.find('{')
            end = text.rfind('}')  # æ‰¾æœ€åä¸€ä¸ª }
            if end > start:
                potential_json = text[start:end + 1]
                return json.loads(potential_json)

        if '[' in text and ']' in text:
            start = text.find('[')
            end = text.rfind(']')
            if end > start:
                potential_json = text[start:end + 1]
                return json.loads(potential_json)
    except:
        pass

    return None


# ================= ä¸»é€»è¾‘ =================
def run():
    print(f"ğŸš€ [Step 2a] AI å¿«é€Ÿåˆ†ç±»å¯åŠ¨ (Model: {config.MODEL_CLASSIFIER})...")

    input_file = config.UNIQUE_LENDERS_FILE
    output_file = config.CLASSIFIED_LENDERS_FILE

    if not os.path.exists(input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ç¼ºå¤±: {input_file}")
        return

    # 1. è¯»å–åå•
    df = pd.read_csv(input_file)
    all_names = [str(n).strip() for n in df['Lender_Name'].dropna().unique() if len(str(n)) > 1]

    # 2. æ–­ç‚¹ç»­ä¼ 
    processed = set()
    if os.path.exists(output_file):
        try:
            processed = set(pd.read_csv(output_file)['name'].astype(str))
            print(f"ğŸ“‚ è·³è¿‡å·²å¤„ç†: {len(processed)} æ¡")
        except:
            pass

    to_process = [n for n in all_names if n not in processed]
    if not to_process:
        print("âœ… æ‰€æœ‰åå•å·²åˆ†ç±»å®Œæ¯•ï¼")
        return

    # 3. å¹¶å‘å¤„ç†
    client = init_client()
    batches = [to_process[i:i + config.BATCH_SIZE_CLASSIFY] for i in
               range(0, len(to_process), config.BATCH_SIZE_CLASSIFY)]

    def process_batch(batch):
        try:
            resp = client.chat.completions.create(
                model=config.MODEL_CLASSIFIER,
                messages=[
                    {"role": "system", "content": PROMPT_CLASSIFY},
                    {"role": "user", "content": "\n".join(batch)}
                ],
                temperature=0.0
            )
            data = parse_json(resp.choices[0].message.content)
            return data.get("results", []) if data else []
        except Exception as e:
            print(f"âš ï¸ Error: {e}")
            return []

    results = []
    with ThreadPoolExecutor(max_workers=config.MAX_WORKERS) as executor:
        futures = {executor.submit(process_batch, batch): batch for batch in batches}

        for future in tqdm(as_completed(futures), total=len(batches), desc="Classifying"):
            res = future.result()
            if res: results.extend(res)

            # æ¯ 5 æ‰¹å­˜ä¸€æ¬¡
            if len(results) >= 250:
                pd.DataFrame(results).to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))
                results = []

    # å­˜å‰©ä½™
    if results:
        pd.DataFrame(results).to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))

    print(f"ğŸ‰ åˆ†ç±»å®Œæˆ: {output_file}")


if __name__ == "__main__":
    run()